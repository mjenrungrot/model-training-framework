# Multi-DataLoader Configuration Examples
# This file demonstrates various multi-dataloader configurations
# Note: The framework is multi-dataloader-only, even for single loader scenarios

# ============================================================================
# EXAMPLE 1: Single DataLoader (wrapped in multi-loader API)
# ============================================================================
single_loader_config:
  multi:
    # For single loader, use SEQUENTIAL strategy
    sampling_strategy: "sequential"
    epoch_length_policy: "sum_of_lengths"
    dataloader_names: ["main"]  # Single name in list
    # No weights or patterns needed for single loader

  validation:
    frequency: "per_epoch"
    aggregation: "micro_avg_weighted_by_samples"

  checkpoint:
    save_every_n_epochs: 1
    max_checkpoints: 5


# ============================================================================
# EXAMPLE 2: Two DataLoaders with Round-Robin
# ============================================================================
round_robin_config:
  multi:
    # Alternates between dataloaders: A, B, A, B, ...
    sampling_strategy: "round_robin"
    epoch_length_policy: "sum_of_lengths"  # Use all data from both
    dataloader_names: ["dataset_a", "dataset_b"]
    cycle_short_loaders: true  # If one runs out, cycle it

  validation:
    frequency: "per_epoch"
    aggregation: "macro_avg_equal_loaders"  # Equal weight to each loader
    per_loader_metrics: true  # Track metrics per loader
    global_metrics: true  # Also track aggregated metrics


# ============================================================================
# EXAMPLE 3: Weighted Sampling (Imbalanced Datasets)
# ============================================================================
weighted_sampling_config:
  multi:
    # Sample based on weights (probabilistic)
    sampling_strategy: "weighted"
    dataloader_weights: [0.6, 0.3, 0.1]  # 60%, 30%, 10% sampling
    epoch_length_policy: "fixed_num_steps"
    steps_per_epoch: 1000  # Fixed 1000 steps per epoch
    dataloader_names: ["large_dataset", "medium_dataset", "small_dataset"]
    choice_rng_seed: 42  # For reproducible weighted sampling

  validation:
    frequency: "every_n_steps"
    every_n_steps: 200
    aggregation: "micro_avg_weighted_by_samples"  # Weight by dataset size

  logging:
    log_loader_proportions: true  # Track actual sampling proportions


# ============================================================================
# EXAMPLE 4: Alternating Pattern (Custom Schedule)
# ============================================================================
alternating_pattern_config:
  multi:
    # Follow explicit pattern
    sampling_strategy: "alternating"
    # Pattern: 2x dataset_a, 1x dataset_b, 1x dataset_c, repeat
    alternating_pattern: [0, 0, 1, 2]  # Indices into dataloader list
    epoch_length_policy: "max_of_lengths"  # Continue until longest is done
    dataloader_names: ["primary", "auxiliary", "validation_like"]
    burst_size: 3  # Take 3 batches at a time from each loader

  performance:
    gradient_accumulation_steps: 4  # Accumulate over pattern


# ============================================================================
# EXAMPLE 5: Sequential Processing (Process One After Another)
# ============================================================================
sequential_config:
  multi:
    # Process loaders sequentially: All of A, then all of B, etc.
    sampling_strategy: "sequential"
    epoch_length_policy: "sum_of_lengths"
    dataloader_names: ["pretraining", "finetuning", "adaptation"]
    # Useful for curriculum learning or staged training

  checkpoint:
    save_every_n_steps: 500  # Save during long sequential processing


# ============================================================================
# EXAMPLE 6: Multi-Task Learning Configuration
# ============================================================================
multi_task_config:
  multi:
    sampling_strategy: "weighted"
    # Different tasks with different importance
    dataloader_weights: [0.4, 0.3, 0.2, 0.1]
    dataloader_names: ["task_a", "task_b", "task_c", "auxiliary"]
    epoch_length_policy: "fixed_num_steps"
    steps_per_epoch: 2000

  validation:
    # Validate each task separately
    aggregation: "primary_metric_per_loader"
    per_loader_metrics: true

  # Per-loader optimizer configuration
  per_loader_optimizers:
    task_a:
      optimizer_idx: 0
      loss_weight: 1.0
    task_b:
      optimizer_idx: 0
      loss_weight: 0.8
    task_c:
      optimizer_idx: 1  # Different optimizer for task_c
      loss_weight: 0.5
    auxiliary:
      optimizer_idx: 0
      loss_weight: 0.1


# ============================================================================
# EXAMPLE 7: DDP Configuration with Multi-Loaders
# ============================================================================
ddp_multi_config:
  multi:
    sampling_strategy: "round_robin"
    dataloader_names: ["shard_1", "shard_2"]
    epoch_length_policy: "sum_of_lengths"

  ddp:
    backend: "nccl"
    sync_schedules_across_ranks: true  # Ensure identical schedules
    validate_schedule_consistency: true  # Runtime validation

  fault_tolerance:
    save_sampler_state: true  # For exact resume
    save_dataset_state: true
    verify_deterministic_resume: true


# ============================================================================
# EXAMPLE 8: Advanced Weighted with Monitoring
# ============================================================================
advanced_weighted_config:
  multi:
    sampling_strategy: "weighted"
    dataloader_weights: [0.5, 0.2, 0.2, 0.1]
    dataloader_names: ["main", "augmented", "hard_negatives", "synthetic"]
    epoch_length_policy: "fixed_num_steps"
    steps_per_epoch: 5000
    choice_rng_seed: 123  # Reproducible sampling
    prefetch_cap_total_batches: 100  # Prefetch limit

  validation:
    frequency: "every_n_steps"
    every_n_steps: 500
    aggregation: "micro_avg_weighted_by_samples"

  logging:
    logger_type: "composite"  # Multiple loggers
    composite_loggers: ["console", "tensorboard", "wandb"]
    log_per_loader_metrics: true
    log_global_metrics: true
    log_loader_proportions: true  # Monitor actual vs expected proportions
    all_reduce_metrics: true  # DDP metric aggregation

  hooks:
    enable_early_stopping_hook: true
    early_stopping_config:
      monitor: "val/loss"
      patience: 10
      mode: "min"
      # Early stopping considers aggregated metrics


# ============================================================================
# EXAMPLE 9: Curriculum Learning with Sequential + Transitions
# ============================================================================
curriculum_config:
  multi:
    sampling_strategy: "sequential"
    dataloader_names: ["easy", "medium", "hard", "expert"]
    epoch_length_policy: "sum_of_lengths"
    # Process in order of difficulty

  # Could use hooks to transition between datasets
  hooks:
    hook_classes: ["custom.CurriculumTransitionHook"]
    hook_configs:
      curriculum:
        transition_epochs: [5, 10, 15]  # When to move to next dataset


# ============================================================================
# EXAMPLE 10: Validation-Only Multi-Loader
# ============================================================================
validation_only_config:
  multi:
    # Even for validation-only, need multi config
    sampling_strategy: "sequential"
    dataloader_names: ["val_iid", "val_ood", "val_adversarial"]
    epoch_length_policy: "sum_of_lengths"

  validation:
    frequency: "per_epoch"
    # Different aggregation for validation diversity
    aggregation: "primary_metric_per_loader"
    per_loader_metrics: true  # Track each validation set separately
